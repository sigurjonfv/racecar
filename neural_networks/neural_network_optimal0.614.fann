FANN_FLO_2.1
num_layers=4
learning_rate=0.010000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=1
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 7 10 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (10, 5, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -7.53904998302459716797e-01) (1, 1.83372125029563903809e-01) (2, -1.11905944347381591797e+00) (3, 1.00627887248992919922e+00) (4, -1.96425831317901611328e+00) (5, 1.35434329509735107422e+00) (6, -3.86715084314346313477e-01) (0, 2.80745863914489746094e+00) (1, 5.89826488494873046875e+00) (2, 1.18296712636947631836e-01) (3, 4.09230738878250122070e-01) (4, 1.20180070400238037109e+00) (5, 1.05468380451202392578e+00) (6, 9.87251698970794677734e-02) (0, -2.95127153396606445312e-01) (1, -6.87888622283935546875e-01) (2, -5.18083237111568450928e-02) (3, -3.95414698868989944458e-03) (4, -2.86039561033248901367e-01) (5, -1.58958896994590759277e-01) (6, 3.14217992126941680908e-02) (0, -1.95311784744262695312e-01) (1, -2.51947999000549316406e-01) (2, -7.12995290756225585938e-01) (3, -3.40493798255920410156e-01) (4, -8.49047005176544189453e-01) (5, -5.46925127506256103516e-01) (6, 4.74767059087753295898e-01) (0, -9.65439021587371826172e-01) (1, 3.15762877464294433594e-01) (2, -9.02916669845581054688e-01) (3, -9.19336318969726562500e-01) (4, -1.37536132335662841797e+00) (5, -3.58831554651260375977e-01) (6, 6.77822232246398925781e-01) (0, 9.81056034564971923828e-01) (1, -1.39582574367523193359e+00) (2, -2.73397695273160934448e-02) (3, 3.28983157873153686523e-01) (4, 2.73053705692291259766e-01) (5, -2.54932552576065063477e-01) (6, 1.68582051992416381836e-02) (7, -3.93974751234054565430e-01) (8, -2.90245950222015380859e-01) (9, -3.18193621933460235596e-02) (10, -7.77834653854370117188e-01) (11, -1.22687928378582000732e-02) (12, -4.67225722968578338623e-02) (13, -7.27189630270004272461e-02) (7, -1.74765303730964660645e-01) (8, -3.10927331447601318359e-01) (9, 8.33575204014778137207e-02) (10, -4.05301719903945922852e-01) (11, 1.41626611351966857910e-01) (12, 2.05981418490409851074e-01) (13, 9.85302329063415527344e-02) (7, 4.45398129522800445557e-02) (8, 1.93543419241905212402e-01) (9, 5.65782114863395690918e-02) (10, 1.38871520757675170898e-01) (11, -5.79581484198570251465e-02) (12, -5.35193830728530883789e-02) (13, -5.16046211123466491699e-02) (7, -1.62315294146537780762e-01) (8, -2.91662663221359252930e-01) (9, -7.50667527318000793457e-02) (10, -3.01359027624130249023e-01) (11, 1.96657031774520874023e-01) (12, 1.06805689632892608643e-01) (13, 6.85542495921254158020e-03) (7, 1.85227259993553161621e-01) (8, 2.37950816750526428223e-01) (9, -4.24781516194343566895e-02) (10, 4.14363533258438110352e-01) (11, -9.17046219110488891602e-02) (12, 5.18194027245044708252e-03) (13, 3.40749062597751617432e-02) (7, -1.90008863806724548340e-01) (8, -2.23303005099296569824e-01) (9, 8.00970792770385742188e-02) (10, -3.50392043590545654297e-01) (11, 1.22182898223400115967e-01) (12, 1.38642892241477966309e-01) (13, 2.27512735873460769653e-02) (7, -1.77446201443672180176e-01) (8, -1.71366348862648010254e-01) (9, -6.46466389298439025879e-02) (10, -3.45209956169128417969e-01) (11, 1.65029749274253845215e-01) (12, 1.74626365303993225098e-01) (13, -1.86386313289403915405e-02) (7, -5.02558387815952301025e-02) (8, -7.15520232915878295898e-02) (9, 2.00529191642999649048e-02) (10, -1.35781345888972282410e-02) (11, 1.15125447511672973633e-01) (12, -1.94158907979726791382e-02) (13, -2.97794938087463378906e-02) (7, -4.48881741613149642944e-03) (8, 4.38771583139896392822e-02) (9, 7.45746791362762451172e-02) (10, 2.97283679246902465820e-02) (11, -5.14409318566322326660e-02) (12, 6.63696154952049255371e-02) (13, 7.48410373926162719727e-02) (14, 4.61216032505035400391e-01) (15, 2.27435111999511718750e-01) (16, -8.74143689870834350586e-02) (17, 1.49966493248939514160e-01) (18, -2.33926877379417419434e-01) (19, 1.81895002722740173340e-01) (20, 1.57243534922599792480e-01) (21, -1.72200575470924377441e-02) (22, -8.65812879055738449097e-03) (23, -4.18046265840530395508e-01) 
