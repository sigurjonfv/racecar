FANN_FLO_2.1
num_layers=4
learning_rate=0.010000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=1
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 7 10 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (10, 5, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -7.48110413551330566406e-01) (1, 2.07023516297340393066e-01) (2, -1.10692930221557617188e+00) (3, 1.01392853260040283203e+00) (4, -1.96390450000762939453e+00) (5, 1.36286866664886474609e+00) (6, -3.86422604322433471680e-01) (0, 2.78674721717834472656e+00) (1, 5.86713552474975585938e+00) (2, 1.28338575363159179688e-01) (3, 4.07918334007263183594e-01) (4, 1.20137274265289306641e+00) (5, 1.05826961994171142578e+00) (6, 9.89027097821235656738e-02) (0, -2.94657438993453979492e-01) (1, -6.86509251594543457031e-01) (2, -5.22063486278057098389e-02) (3, -3.90304508619010448456e-03) (4, -2.85979270935058593750e-01) (5, -1.58996254205703735352e-01) (6, 3.14195901155471801758e-02) (0, -2.17471256852149963379e-01) (1, -2.54333138465881347656e-01) (2, -7.20563769340515136719e-01) (3, -3.74907225370407104492e-01) (4, -8.22201251983642578125e-01) (5, -5.16573607921600341797e-01) (6, 4.77156877517700195312e-01) (0, -9.77057278156280517578e-01) (1, 3.14480245113372802734e-01) (2, -9.01340425014495849609e-01) (3, -8.81293594837188720703e-01) (4, -1.37987458705902099609e+00) (5, -3.73794853687286376953e-01) (6, 6.74718141555786132812e-01) (0, 9.80490088462829589844e-01) (1, -1.39600801467895507812e+00) (2, -3.60475145280361175537e-02) (3, 3.25894474983215332031e-01) (4, 2.69679397344589233398e-01) (5, -2.49300941824913024902e-01) (6, 1.35031538084149360657e-02) (7, -3.88835221529006958008e-01) (8, -2.99444556236267089844e-01) (9, -2.27750204503536224365e-02) (10, -7.72007822990417480469e-01) (11, 1.41057576984167098999e-02) (12, -9.71405431628227233887e-02) (13, -8.18394869565963745117e-02) (7, -1.74734547734260559082e-01) (8, -3.14353436231613159180e-01) (9, 8.66623446345329284668e-02) (10, -4.04454678297042846680e-01) (11, 1.54231771826744079590e-01) (12, 1.75214082002639770508e-01) (13, 9.51247438788414001465e-02) (7, 4.42896857857704162598e-02) (8, 1.94826781749725341797e-01) (9, 5.52858449518680572510e-02) (10, 1.38608455657958984375e-01) (11, -6.32545948028564453125e-02) (12, -4.15209643542766571045e-02) (13, -5.02838604152202606201e-02) (7, -1.61909356713294982910e-01) (8, -2.94155508279800415039e-01) (9, -7.27479830384254455566e-02) (10, -3.00930559635162353516e-01) (11, 2.05484226346015930176e-01) (12, 8.59175324440002441406e-02) (13, 4.46845032274723052979e-03) (7, 1.83079719543457031250e-01) (8, 2.41836294531822204590e-01) (9, -4.62740361690521240234e-02) (10, 4.13063466548919677734e-01) (11, -1.07023991644382476807e-01) (12, 3.85868549346923828125e-02) (13, 3.79300862550735473633e-02) (7, -1.89779788255691528320e-01) (8, -2.26054221391677856445e-01) (9, 8.28229561448097229004e-02) (10, -3.49729895591735839844e-01) (11, 1.32594823837280273438e-01) (12, 1.13254465162754058838e-01) (13, 1.99750680476427078247e-02) (7, -1.77853256464004516602e-01) (8, -1.73752307891845703125e-01) (9, -6.23470991849899291992e-02) (10, -3.44717770814895629883e-01) (11, 1.72994449734687805176e-01) (12, 1.53832376003265380859e-01) (13, -2.10139770060777664185e-02) (7, -5.01084290444850921631e-02) (8, -7.12029635906219482422e-02) (9, 1.97328682988882064819e-02) (10, -1.40296407043933868408e-02) (11, 1.14585220813751220703e-01) (12, -1.79086644202470779419e-02) (13, -2.94356476515531539917e-02) (7, -5.07418485358357429504e-03) (8, 4.38184961676597595215e-02) (9, 7.45954960584640502930e-02) (10, 2.99743786454200744629e-02) (11, -5.28096333146095275879e-02) (12, 6.83018863201141357422e-02) (13, 7.48212113976478576660e-02) (14, 4.67222154140472412109e-01) (15, 2.12357506155967712402e-01) (16, -8.53238254785537719727e-02) (17, 1.50837346911430358887e-01) (18, -2.45029225945472717285e-01) (19, 1.74301877617835998535e-01) (20, 1.45097658038139343262e-01) (21, -5.44973229989409446716e-03) (22, -2.21738442778587341309e-02) (23, -4.47584420442581176758e-01) 
