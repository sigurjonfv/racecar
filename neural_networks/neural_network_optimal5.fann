FANN_FLO_2.1
num_layers=4
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=1
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 7 10 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (10, 5, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -7.76425376534461975098e-02) (1, 4.18347030878067016602e-01) (2, 8.19154083728790283203e-02) (3, 3.39986830949783325195e-01) (4, 1.15017734467983245850e-01) (5, 2.68810570240020751953e-01) (6, -1.26945585012435913086e-01) (0, -4.14408296346664428711e-01) (1, -1.32983401417732238770e-02) (2, -4.53674793243408203125e-01) (3, -6.08661361038684844971e-02) (4, -5.37223398685455322266e-01) (5, -8.54623541235923767090e-02) (6, -1.09134435653686523438e-01) (0, -4.58647869527339935303e-02) (1, 9.49373096227645874023e-02) (2, 5.20046651363372802734e-02) (3, 1.68938860297203063965e-02) (4, -9.85093414783477783203e-03) (5, -2.16765388846397399902e-01) (6, 4.10664677619934082031e-02) (0, 9.26800668239593505859e-02) (1, -5.09954094886779785156e-02) (2, 9.86800044775009155273e-02) (3, 5.29236532747745513916e-02) (4, 7.21428096294403076172e-02) (5, 2.28194728493690490723e-01) (6, -6.19532316923141479492e-02) (0, -1.38091102242469787598e-01) (1, -2.37552821636199951172e-01) (2, 5.11559918522834777832e-02) (3, 3.86029966175556182861e-02) (4, 2.93820649385452270508e-01) (5, -6.01870566606521606445e-02) (6, 3.54759655892848968506e-02) (0, -5.48818372189998626709e-02) (1, 2.79954522848129272461e-01) (2, -8.13918635249137878418e-02) (3, 1.96408584713935852051e-01) (4, -3.45966182649135589600e-02) (5, -1.01789981126785278320e-01) (6, 2.70744785666465759277e-02) (7, 1.27618551254272460938e-01) (8, -1.69306382536888122559e-01) (9, -1.35512784123420715332e-01) (10, -3.03961187601089477539e-02) (11, -1.82080194354057312012e-02) (12, -1.14218100905418395996e-01) (13, -6.63810148835182189941e-02) (7, -6.18410818278789520264e-02) (8, 1.09167560935020446777e-01) (9, -1.76057871431112289429e-03) (10, -1.47192412987351417542e-02) (11, 7.47278481721878051758e-02) (12, -6.48383125662803649902e-02) (13, 1.07144139707088470459e-01) (7, 4.94418591260910034180e-02) (8, -1.32946595549583435059e-01) (9, -7.54863314796239137650e-04) (10, -5.10013476014137268066e-03) (11, 4.88152876496315002441e-02) (12, -9.35283675789833068848e-02) (13, 8.66450667381286621094e-02) (7, 6.17018006742000579834e-02) (8, -9.03523415327072143555e-02) (9, -5.60484193265438079834e-02) (10, 2.99519505351781845093e-02) (11, -3.01957014016807079315e-03) (12, -9.24827307462692260742e-02) (13, -9.86455678939819335938e-02) (7, 2.92084246873855590820e-01) (8, -2.39051222801208496094e-01) (9, -2.58470147848129272461e-01) (10, -2.52199620008468627930e-01) (11, 3.59668880701065063477e-02) (12, -1.02241016924381256104e-01) (13, -6.14912547171115875244e-02) (7, -1.06889411807060241699e-01) (8, 7.28464052081108093262e-02) (9, -1.99230946600437164307e-02) (10, -1.58623978495597839355e-02) (11, -4.62545715272426605225e-02) (12, 4.82206270098686218262e-02) (13, 7.37028717994689941406e-02) (7, -1.46742373704910278320e-01) (8, 2.07076281309127807617e-01) (9, 9.27002727985382080078e-02) (10, 1.65719121694564819336e-01) (11, 6.93397521972656250000e-02) (12, 1.90892085433006286621e-01) (13, 5.31788840889930725098e-02) (7, 1.42984241247177124023e-01) (8, -9.79249402880668640137e-02) (9, -4.07612510025501251221e-02) (10, -8.64745676517486572266e-02) (11, -7.03786537051200866699e-02) (12, -1.22286386787891387939e-01) (13, 5.34442402422428131104e-02) (7, 2.32654660940170288086e-01) (8, -3.05883169174194335938e-01) (9, -2.08588108420372009277e-01) (10, -1.86756536364555358887e-01) (11, -1.00165203213691711426e-01) (12, -9.52802449464797973633e-02) (13, -5.17720058560371398926e-02) (14, 2.48326376080513000488e-01) (15, -8.83468389511108398438e-02) (16, 1.01072341203689575195e-01) (17, 1.31428346037864685059e-01) (18, 5.06044447422027587891e-01) (19, -8.23134854435920715332e-02) (20, -3.56057822704315185547e-01) (21, 2.37871736288070678711e-01) (22, 4.83034163713455200195e-01) (23, -2.42382034659385681152e-01) 
