FANN_FLO_2.1
num_layers=4
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=1
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 7 10 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (10, 5, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.03862881660461425781e-01) (1, -7.60751171037554740906e-03) (2, -4.25217933952808380127e-02) (3, 7.30028226971626281738e-02) (4, -1.21913023293018341064e-01) (5, 2.25231230258941650391e-01) (6, 1.38302907347679138184e-01) (0, -4.21392470598220825195e-01) (1, 1.96319688111543655396e-02) (2, -4.57403779029846191406e-01) (3, 5.64889907836914062500e-02) (4, -6.07482254505157470703e-01) (5, 4.30246293544769287109e-02) (6, 8.28219875693321228027e-02) (0, -1.06229007244110107422e-01) (1, 2.78729587793350219727e-01) (2, -1.33571252226829528809e-01) (3, 2.51407027244567871094e-01) (4, -3.33959817886352539062e-01) (5, -6.21539466083049774170e-02) (6, 7.32380226254463195801e-02) (0, -3.77865701913833618164e-01) (1, -3.68946254253387451172e-01) (2, -1.10941775143146514893e-01) (3, -1.50503339245915412903e-02) (4, -8.07209964841604232788e-03) (5, 5.31730316579341888428e-02) (6, 2.27510631084442138672e-02) (0, 6.38673838693648576736e-04) (1, 2.45026145130395889282e-02) (2, -1.69894814491271972656e-01) (3, -1.91132456064224243164e-01) (4, -1.30475908517837524414e-01) (5, -5.25315463542938232422e-01) (6, 8.36395248770713806152e-02) (0, 2.46959980577230453491e-02) (1, 3.23004961013793945312e-01) (2, 9.46876704692840576172e-02) (3, 4.14718955755233764648e-01) (4, -4.94711287319660186768e-02) (5, 2.57980614900588989258e-01) (6, 3.93555536866188049316e-02) (7, 5.46929351985454559326e-02) (8, -1.59263551235198974609e-01) (9, -6.89247325062751770020e-02) (10, 4.51577119529247283936e-02) (11, -2.25130580365657806396e-02) (12, 6.81086182594299316406e-02) (13, -1.02050662040710449219e-01) (7, 2.32789859175682067871e-01) (8, -1.91872090101242065430e-01) (9, 1.36783542111515998840e-02) (10, 8.78329500555992126465e-02) (11, 1.62604600191116333008e-01) (12, 2.88986563682556152344e-01) (13, 5.57047240436077117920e-02) (7, -2.63180524110794067383e-01) (8, 1.92149817943572998047e-01) (9, 1.68425500392913818359e-01) (10, -1.07416734099388122559e-01) (11, -1.10394179821014404297e-01) (12, -2.75631606578826904297e-01) (13, -3.64677906036376953125e-02) (7, 3.10538262128829956055e-01) (8, -2.36309319734573364258e-01) (9, -2.15648800134658813477e-01) (10, 1.42940789461135864258e-01) (11, 2.09310322999954223633e-01) (12, 3.43772709369659423828e-01) (13, -1.50756277143955230713e-02) (7, -4.24122996628284454346e-02) (8, 8.18691775202751159668e-02) (9, 3.30570302903652191162e-02) (10, -3.76792512834072113037e-02) (11, 1.16547765210270881653e-02) (12, -2.64755934476852416992e-02) (13, 1.15694953128695487976e-02) (7, 3.89506155624985694885e-03) (8, -9.32772159576416015625e-02) (9, 8.77987816929817199707e-02) (10, 1.18663311004638671875e-02) (11, 6.12649843096733093262e-02) (12, 4.14570569992065429688e-02) (13, 1.92131567746400833130e-02) (7, 3.96688543260097503662e-02) (8, -1.60135760903358459473e-01) (9, -1.04756772518157958984e-01) (10, -4.03074882924556732178e-02) (11, 1.30271077156066894531e-01) (12, 1.15126639604568481445e-01) (13, -3.44305182807147502899e-03) (7, -7.16167315840721130371e-02) (8, 6.94400146603584289551e-02) (9, 1.04802764952182769775e-01) (10, 2.25210227072238922119e-02) (11, 4.47192713618278503418e-02) (12, -2.14578405022621154785e-01) (13, -4.48152907192707061768e-02) (7, 7.00046867132186889648e-02) (8, -2.74863373488187789917e-02) (9, 7.25361937656998634338e-03) (10, 3.50711159408092498779e-02) (11, 2.15534344315528869629e-02) (12, 2.35888779163360595703e-01) (13, 6.37617856264114379883e-02) (14, 1.77292287349700927734e-01) (15, 4.06847804784774780273e-01) (16, -4.50614750385284423828e-01) (17, 5.81091880798339843750e-01) (18, -1.01312957704067230225e-01) (19, 5.38986064493656158447e-02) (20, 1.93976759910583496094e-01) (21, -2.15124383568763732910e-01) (22, 2.09503114223480224609e-01) (23, -2.51682072877883911133e-01) 
