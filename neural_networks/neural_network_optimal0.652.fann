FANN_FLO_2.1
num_layers=4
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=1
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=7 7 10 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (7, 5, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (10, 5, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.85787186026573181152e-01) (1, -2.33759790658950805664e-01) (2, -2.75024384260177612305e-01) (3, -3.64924043416976928711e-01) (4, -4.57971602678298950195e-01) (5, -6.01112067699432373047e-01) (6, 2.27393686771392822266e-01) (0, 5.88792860507965087891e-01) (1, 1.18542850017547607422e+00) (2, -2.17078208923339843750e-01) (3, 1.25267237424850463867e-01) (4, 3.89228343963623046875e-01) (5, 5.92546701431274414062e-01) (6, 1.54508486390113830566e-01) (0, 1.07507169246673583984e+00) (1, -7.99752116203308105469e-01) (2, 2.77162671089172363281e-01) (3, 1.54177948832511901855e-01) (4, 2.07181259989738464355e-01) (5, 4.87927794456481933594e-01) (6, 1.24434053897857666016e-01) (0, -4.97777760028839111328e-01) (1, 1.47018700838088989258e-01) (2, -2.37324908375740051270e-01) (3, -5.39971530437469482422e-01) (4, -4.67339932918548583984e-01) (5, -7.59265661239624023438e-01) (6, -1.81336589157581329346e-02) (0, 5.85462987422943115234e-01) (1, -3.67038339376449584961e-01) (2, -1.57312884926795959473e-01) (3, 2.31534093618392944336e-01) (4, 3.10500394552946090698e-02) (5, -3.52203667163848876953e-01) (6, -2.40026101469993591309e-01) (0, -1.04119241237640380859e+00) (1, 1.59577623009681701660e-01) (2, -9.91382718086242675781e-01) (3, 1.72458365559577941895e-02) (4, -1.21139788627624511719e+00) (5, -3.39679270982742309570e-01) (6, 2.39462256431579589844e-01) (7, -3.96378427743911743164e-01) (8, -2.51014709472656250000e-01) (9, 3.21691840887069702148e-01) (10, 5.30120655894279479980e-02) (11, -4.88514572381973266602e-01) (12, -4.01342868804931640625e-01) (13, -1.78614974021911621094e-01) (7, -2.18812033534049987793e-01) (8, -2.22883582115173339844e-01) (9, 2.88269460201263427734e-01) (10, 1.40702277421951293945e-01) (11, -1.80995285511016845703e-01) (12, -1.70228093862533569336e-01) (13, 5.35837970674037933350e-02) (7, 9.63078066706657409668e-02) (8, 1.52847453951835632324e-01) (9, -2.93821543455123901367e-02) (10, -9.54567864537239074707e-02) (11, 1.22850723564624786377e-01) (12, 1.21153272688388824463e-01) (13, -3.74222584068775177002e-02) (7, -2.31599286198616027832e-01) (8, -2.25600659847259521484e-01) (9, 1.01828098297119140625e-01) (10, 1.59639179706573486328e-01) (11, -1.80353626608848571777e-01) (12, -2.57376283407211303711e-01) (13, -3.93121540546417236328e-02) (7, 2.63949424028396606445e-01) (8, 1.60748794674873352051e-01) (9, -2.49256893992424011230e-01) (10, -6.64966329932212829590e-02) (11, 3.22372138500213623047e-01) (12, 3.03982198238372802734e-01) (13, 7.41175860166549682617e-02) (7, -2.22062408924102783203e-01) (8, -1.53727278113365173340e-01) (9, 2.49760881066322326660e-01) (10, 5.63753657042980194092e-02) (11, -1.58231064677238464355e-01) (12, -1.46085336804389953613e-01) (13, 6.34445052128285169601e-04) (7, -1.37422427535057067871e-01) (8, -1.26760050654411315918e-01) (9, 6.91328058019280433655e-03) (10, -6.33892267942428588867e-02) (11, 6.94043235853314399719e-03) (12, -6.42702281475067138672e-02) (13, 1.52340214699506759644e-02) (7, -9.40623655915260314941e-02) (8, -5.44630661606788635254e-02) (9, 9.09464284777641296387e-02) (10, 9.75068584084510803223e-02) (11, -3.15614044666290283203e-02) (12, -1.44580557942390441895e-01) (13, -7.38820359110832214355e-02) (7, 1.75969991832971572876e-02) (8, 4.19137366116046905518e-02) (9, 5.99313527345657348633e-02) (10, 1.68388281017541885376e-02) (11, 2.25993674248456954956e-02) (12, 1.05706676840782165527e-01) (13, 7.34298676252365112305e-02) (14, 7.62694716453552246094e-01) (15, 4.06493484973907470703e-01) (16, -1.94137081503868103027e-01) (17, 3.86122167110443115234e-01) (18, -5.31054139137268066406e-01) (19, 3.56865823268890380859e-01) (20, 1.06273353099822998047e-01) (21, 1.52048274874687194824e-01) (22, -3.85070443153381347656e-02) (23, -5.71083247661590576172e-01) 
